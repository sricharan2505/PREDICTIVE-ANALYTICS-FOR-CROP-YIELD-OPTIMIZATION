# -*- coding: utf-8 -*-
"""RF(CAPSTONE)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jwKHWZKlMxTFnADQlNlqiKEwneeRxICX

Step 1: Importing Necessary Libraries

pandas → For handling the dataset.

numpy → For numerical computations.

matplotlib & seaborn → For data visualization.

train_test_split → To split the dataset into training & testing.

StandardScaler → To normalize the feature values.

RandomForestRegressor → To train the Random Forest Model.

r2_score, mean_absolute_error, mean_squared_error → To evaluate model performance.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

"""Step 2: Load the Dataset"""

import pandas as pd

# Load dataset
file_path = '/content/kharif(capstone project_dataset).xlsx'
data = pd.read_excel(file_path)

# Display the first 5 rows of the dataset
print(data.head())

print(data.describe)

print(data.isnull())

print(data.dtypes)

""" Step 3: Selecting Features & Target Variable"""

# Clean column names (remove extra spaces)
data.columns = data.columns.str.strip()

# Drop unnecessary categorical columns
# Define Features (X) - Excluding the target variable
X = data.drop(columns=['State', 'District', 'Year Range','Kharif Yield (Tonne/Hectare)'])

# Define Target (y) - The variable we want to predict
y = data['Kharif Yield (Tonne/Hectare)']

# Display first few rows of X and y
print("Features (X):")
print(X.head())

print("\nTarget (y):")
print(y.head())

"""Explanation:

X (Features)
We remove columns that are not useful for prediction:
"State", "District", "Year Range" → These are categorical and not numerically meaningful.

"Kharif Yield (Tonne/Hectare)" → This is the target variable, so we remove it from X.

y (Target)
The target variable (what we are predicting) is "Kharif Yield (Tonne/Hectare)".

Step 4: Data Preprocessing (Handling Missing Values)
"""

# Check for missing values
print(data.isnull().sum())

# Calculate the mean of numeric columns only
numeric_means = data.mean(numeric_only=True)

# Fill missing values in numeric columns with their respective means
data.fillna(numeric_means, inplace=True)

"""explanation:

df.isnull().sum() → Checks how many missing values are present.


df.fillna(df.mean(), inplace=True) → Fills missing values with the column mean.

visulization of the dataset
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np # Import numpy

# Correlation matrix
plt.figure(figsize=(14, 10))
sns.heatmap(data.select_dtypes(include=np.number).corr(), annot=True, cmap='coolwarm') # Select only numeric columns
plt.title('Correlation Heatmap')
plt.show()

# Check the data types to identify non-numeric columns
print(data.dtypes)

# Identify non-numeric columns
non_numeric_columns = data.select_dtypes(exclude=['number']).columns

# Drop non-numeric columns
data = data.drop(columns=non_numeric_columns)

# Now calculate the correlation matrix
data.corr()

"""Correlation Matrix

A correlation matrix is a table that shows the correlation coefficients between multiple variables in a dataset. It helps identify relationships between variables, ranging from -1 to 1:


1 → Perfect positive correlation (variables increase together).

0 → No correlation (variables are independent).

-1 → Perfect negative correlation (one variable increases while the other decreases).
"""

# Histogram for Kharif Yield
plt.hist(data['Kharif Yield (Tonne/Hectare)'], bins=30, color='green', edgecolor='black')
plt.title('Kharif Yield Distribution')
plt.xlabel('Yield (Tonne/Hectare)')
plt.ylabel('Frequency')
plt.show()

sns.scatterplot(x=data['Kharif Rainfall (in mm)'], y=data['Kharif Yield (Tonne/Hectare)'])
plt.title('Kharif Rainfall vs Yield')
plt.xlabel('Kharif Rainfall (in mm)')
plt.ylabel('Kharif Yield (Tonne/Hectare)')
plt.show()

# Define features (X) and target variable (y)
X = data.drop(columns=['Kharif Yield (Tonne/Hectare)'])  # Drop target column
y = data['Kharif Yield (Tonne/Hectare)']  # Target variable


# Display selected features
print(X.head())
print(y.head())

"""Step 5: Normalize the Data

Standardization (Normalization) ensures that all numerical features have the same scale, which improves model accuracy.
StandardScaler() transforms data so that:
Mean = 0
Standard Deviation = 1
fit_transform(X) applies this scaling to our features (X).
"""

# Standardize features (Z-score normalization)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""Explanation:

Why Normalize?
Some features may have large values (e.g., Rainfall in mm) while others have small values (e.g., Zinc %).

Standardization scales all values to have mean = 0 and standard deviation = 1.

How?

StandardScaler() → Creates a scaler object.
fit_transform(X) → Computes the scaling factors from X and applies them.

Step 6: Split the Data Into Training & Testing Sets
"""

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""Explanation:

Why Split?

We need to train the model on one set and test it on another to measure performance.

How?

test_size=0.2 → 20% of the data is used for testing, 80% for training.
random_state=42 → Ensures the split is consistent every time you run the code.

Step 7: Train the Random Forest Model
"""

from sklearn.ensemble import RandomForestRegressor

# Train the Random Forest Model
rf_model = RandomForestRegressor(n_estimators=150, max_depth=10, random_state=42)
rf_model.fit(X_train, y_train)

"""Explanation:

What is Random Forest?

It is an ensemble learning method that builds multiple decision trees and combines their predictions.


Hyperparameters Used:

n_estimators=200 → The model creates 150 decision trees.

max_depth=10 → Limits the depth of each tree to prevent overfitting.

random_state=42 →it controls the randomness in how the algorithm builds decision trees.

The randomness comes from:

Bootstrapping (Random Sampling of Data)

Feature Selection (Random Subset of Features for Splitting)

Step 8: Make Predictions
"""

# Predict on test data
y_pred = rf_model.predict(X_test)

"""Explanation:
Uses the trained model to predict Kharif Yield (Tonne/Hectare) for the test dataset.

Step 9: Evaluate the Model
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred = rf_model.predict(X_test)

y_test = np.ravel(y_test)
y_pred = np.ravel(y_pred)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Calculate Mean Absolute Percentage Error (MAPE)
mape = (np.abs((y_test - y_pred) / y_test)).mean() * 100

# Calculate accuracy as (100 - MAPE)
accuracy = 100 - mape

print(f"Mean Squared Error (MSE): {mse:.5f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.5f}")
print(f"Mean Absolute Error (MAE): {mae:.5f}")
print(f"R² Score: {r2:.5f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"Accuracy: {accuracy:.2f}%")

"""EXPLANATION

The trained Random Forest model (rf_model) is used to make predictions on the test dataset (X_test).

The predicted values (y_pred) represent the estimated crop yield for each test case.

Sometimes, y_test and y_pred are stored as multi-dimensional arrays (e.g., [[4.5], [5.2], [3.8]] instead of [4.5, 5.2, 3.8]).

We convert them into 1D arrays to ensure compatibility with performance metric calculations.

MSE measures the average squared difference between the actual (y_test) and predicted (y_pred) values.

RMSE is the square root of MSE, making it easier to interpret because it's in the same unit as the target variable.

MAE measures the average absolute difference between actual and predicted values.

R² measures how well the model explains the variance in the target variable.

Key points:

R² = 1 → Perfect model (100% variance explained).


R² = 0 → Model is as good as predicting the mean of y_test.

R² < 0 → Model is worse than just guessing the mean.

Higher R² means a better fit.

MAPE calculates the percentage error between actual and predicted values.

Formula:


Accuracy=100−MAPE

Key points:

If MAPE = 15%, accuracy = 100 - 15 = 85%.

If MAPE = 25%, accuracy = 100 - 25 = 75%.

Higher accuracy = better predictions.

step:9 testing the model with new data
"""

import numpy as np

# Feature importance
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], color="skyblue", align="center")
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.show()

# Create new data dynamically based on training features
new_data = pd.DataFrame([[10060, 200, 19, 140, 1.5, 0.8, 1.2,3.5,2.1,0.5,0.3,0.7]], columns=X.columns)

# Ensure that new_data columns match exactly with the training feature names
print("Training Features:", list(X.columns))
print("New Data Features:", list(new_data.columns))

# Scale the new data
new_data_scaled = scaler.transform(new_data)

# Predict yield using the trained Random Forest model
forecasted_yield = rf_model.predict(new_data_scaled)
print(f"Forecasted Yield: {forecasted_yield[0] * 100:.5f}% Tonne/Hectare")